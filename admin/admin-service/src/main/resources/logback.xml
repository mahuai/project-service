<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <!-- %m输出的信息,%p日志级别,%t线程名,%d日期,%c类的全名,%i索引【从数字0开始递增】,,, -->
    <!-- appender是configuration的子节点，是负责写日志的组件。 -->
    <!-- ConsoleAppender：把日志输出到控制台 -->
    <property name="log_path" value="/logs/admin"></property>
    <property name="appName" value="admin"/>
    <property name="maxHistory" value="30"/>
    <property name="maxFileSize" value="1MB"/><!-- 单个文件最大大小 -->
    <property name="Kafka_topic" value="project-topic" />
    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%d{"yyyy-MM-dd HH:mm:ss,SSS"}[%X{userId}|%X{sessionId}][%p] [%c{0}-%M]-%m%n</pattern>
            <!-- 控制台也要使用UTF-8，不要使用GBK，否则会中文乱码 -->
            <charset>UTF-8</charset>
        </encoder>
    </appender>
    <!-- RollingFileAppender：滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件 -->
    <!-- 以下的大概意思是：1.先按日期存日志，日期变了，将前一天的日志文件名重命名为XXX%日期%索引，新的日志仍然是demo.log -->
    <!--             2.如果日期没有发生变化，但是当前日志的文件大小超过1KB时，对当前日志进行分割 重命名-->
    <appender name="rollingFileError" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <!--<File>${log_path}/${appName}.log</File>-->
        <!-- rollingPolicy:当发生滚动时，决定 RollingFileAppender 的行为，涉及文件移动和重命名。 -->
        <!-- TimeBasedRollingPolicy： 最常用的滚动策略，它根据时间来制定滚动策略，既负责滚动也负责出发滚动 -->
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!-- 活动文件的名字会根据fileNamePattern的值，每隔一段时间改变一次 -->
            <fileNamePattern>${log_path}/${appName}-log-info-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
            <!-- 每产生一个日志文件，该日志文件的保存期限为30天 -->
            <maxHistory>${maxHistory}</maxHistory>
            <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <!-- maxFileSize:这是活动文件的大小，默认值是10MB，测试时可改成1KB看效果 -->
                <maxFileSize>${maxFileSize}</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder>
            <!-- pattern节点，用来设置日志的输入格式 -->
            <pattern>%d{"yyyy-MM-dd HH:mm:ss,SSS"}[%X{userId}|%X{sessionId}][%p] [%c{0}-%M]-%m%n</pattern>
            <!-- 记录日志的编码:此处设置字符集 - -->
            <charset>UTF-8</charset>
        </encoder>
        <!--日志级别-->
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>ERROR</level>
            <onMatch>DENY</onMatch>
            <onMismatch>ACCEPT</onMismatch>
        </filter>
    </appender>

    <!-- This is the kafkaAppender -->
    <!--<appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        &lt;!&ndash; This is the default encoder that encodes every log message to an utf8-encoded string  &ndash;&gt;
        <encoder class="com.github.danielwegener.logback.kafka.encoding.LayoutKafkaMessageEncoder">
            <layout class="ch.qos.logback.classic.PatternLayout">
                <pattern>
                    <pattern>
                        <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS}###${appName}###%level###%logger{50}:%line###%msg###%n
                        </pattern>
                    </pattern>
                </pattern>
            </layout>
        </encoder>
        <topic>${Kafka_topic}</topic>
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.RoundRobinKeyingStrategy"/>
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>

        &lt;!&ndash; each <producerConfig> translates to regular kafka-client config (format: key=value) &ndash;&gt;
        &lt;!&ndash; producer configs are documented here: https://kafka.apache.org/documentation.html#newproducerconfigs &ndash;&gt;
        &lt;!&ndash; bootstrap.servers is the only mandatory producerConfig &ndash;&gt;
        <producerConfig>bootstrap.servers=localhost:9092</producerConfig>

        &lt;!&ndash; this is the fallback appender if kafka is not available. &ndash;&gt;
        <appender-ref ref="STDOUT"/>
    </appender>-->
    <!--myibatis log configure-->
    <logger name="org.apache.ibatis" level="TRACE"/>
    <logger name="java.sql.Connection" level="DEBUG"/>
    <logger name="java.sql.Statement" level="DEBUG"/>
    <logger name="java.sql.PreparedStatement" level="DEBUG"/>
    <!-- 控制台输出日志级别 -->
    <root level="info">
        <appender-ref ref="STDOUT"/>
        <appender-ref ref="rollingFileError"/>
        <!--<appender-ref ref="kafkaAppender"/>-->
    </root>
    <!-- 指定项目中某个包，当有日志操作行为时的日志记录级别 -->
    <!-- com.liyan为根包，也就是只要是发生在这个根包下面的所有日志操作行为的权限都是DEBUG -->
    <!-- 级别依次为【从高到低】：FATAL > ERROR > WARN > INFO > DEBUG > TRACE  -->
    <!--<logger name="com.admin.service" level="DEBUG">
        <appender-ref ref="rollingFileError"/>
    </logger>-->
</configuration>